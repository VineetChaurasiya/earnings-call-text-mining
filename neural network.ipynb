{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchtext\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(BHF).txt',\n",
       " '(BTU).txt',\n",
       " '(BXE).txt',\n",
       " '(CCFI).txt',\n",
       " '(CHC).txt',\n",
       " '(CLUB).txt',\n",
       " '(CVO).txt',\n",
       " '(CYH).txt',\n",
       " '(EVEP).txt',\n",
       " '(FE).txt',\n",
       " '(FELP).txt',\n",
       " '(FMSA).txt',\n",
       " '(FWM).txt',\n",
       " '(GST).txt',\n",
       " '(HERO).txt',\n",
       " '(I).txt',\n",
       " '(ICC).txt',\n",
       " '(ICON).txt',\n",
       " '(IHRT).txt',\n",
       " '(ISH).txt',\n",
       " '(LEGCY).txt',\n",
       " '(MFRM).txt',\n",
       " '(NES).txt',\n",
       " '(PGN).txt',\n",
       " '(PKD).txt',\n",
       " '(PQ).txt',\n",
       " '(SFX).txt',\n",
       " '(SHLD).txt',\n",
       " '(WIN).txt',\n",
       " '(WLB).txt',\n",
       " '.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " '._(BTU).txt',\n",
       " '._(CHC).txt',\n",
       " '._(HERO).txt',\n",
       " '._(PGN).txt',\n",
       " '._.DS_Store',\n",
       " 'preprocess']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "def load_data(data, label):\n",
    "    for file in os.listdir(label):\n",
    "        if file.endswith('.txt') and not file.startswith('.'):\n",
    "            file_path = label + '/' + file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data[text] = label\n",
    "df = {}\n",
    "load_data(df, 'default')\n",
    "load_data(df, 'ndefault')\n",
    "train_df = {}\n",
    "test_df = {}\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 17\n"
     ]
    }
   ],
   "source": [
    "import random, time\n",
    "random.seed(123)\n",
    "split = random.sample(list(df), k=int(len(df)*0.8))\n",
    "for s in split:\n",
    "    train_df[s] = df[s]\n",
    "for text in df:\n",
    "    if text not in train_df:\n",
    "        test_df[text] = df[text]\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_HELDOUT_PATH = \"pred_nn.txt\"\n",
    "\n",
    "idx2label = [\"default\", \"ndefault\"]\n",
    "label2idx = {label: idx for idx, label in enumerate(idx2label)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRunner(object):\n",
    "    def __init__(self, data, voca_size, in_dim, hid_dim, word_embedding):\n",
    "        self.data = data\n",
    "        self.clf = Classifier(voca_size, in_dim, hid_dim, word_embedding).to(device)\n",
    "        self.optimizer = optim.Adam(self.clf.parameters())\n",
    "        # criterion\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        #self.ce_loss = nn.PoissonNLLLoss()\n",
    "\n",
    "    def run_epoch(self, split):\n",
    "        \"\"\"Runs an epoch, during which the classifier is trained or applied\n",
    "        on the data. Returns the predicted labels of the instances.\"\"\"\n",
    "\n",
    "        if split == \"dev\": self.clf.train()\n",
    "        else: self.clf.eval()\n",
    "        epoch_loss = 0\n",
    "        labels_pred = []\n",
    "        for i, (words, label) in enumerate(self.data[split]):\n",
    "            #m = nn.Dropout(p=0.9, inplace = True)\n",
    "            logit = self.clf(torch.LongTensor(words).cuda())\n",
    "            #logit = m(logit)\n",
    "            # Optimize\n",
    "            if split == \"dev\":\n",
    "                loss = self.ce_loss(logit, torch.LongTensor([label]).cuda())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                #print(loss.item())\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            idx = torch.argmax(logit)\n",
    "            #print(idx)\n",
    "            #print(\"------------\")\n",
    "            \n",
    "            labels_pred.append(idx2label[idx])\n",
    "        if split == \"test\":\n",
    "            tp_default = 0\n",
    "            tp_ndefault = 0\n",
    "            total_default = 0\n",
    "            total_ndefault = 0\n",
    "            predicted_default = 0\n",
    "            predicted_ndefault = 0\n",
    "            total = len(self.data[split])\n",
    "\n",
    "            for i in range(len(self.data[split])):\n",
    "                _, test_label = self.data[split][i]\n",
    "                \n",
    "                print(\"Lable:\", test_label)\n",
    "                print(\"Pridicted:\", labels_pred[i])\n",
    "                \n",
    "                if test_label == \"default\":\n",
    "                    total_default += 1\n",
    "                    if labels_pred[i] == \"default\":\n",
    "                        tp_default += 1\n",
    "                else:\n",
    "                    total_ndefault += 1\n",
    "                    if labels_pred[i] == \"ndefault\":\n",
    "                        tp_ndefault += 1\n",
    "                if labels_pred[i] == \"default\":\n",
    "                    predicted_default += 1\n",
    "                else:\n",
    "                    predicted_ndefault += 1\n",
    "            print(\"Overall accuracy:\", (tp_default+tp_ndefault)/total)\n",
    "            print(\"Precision for default:\", tp_default/total_default)\n",
    "            print(\"Precision for ndefault:\", tp_ndefault/total_ndefault)\n",
    "            if predicted_default != 0:\n",
    "                print(\"Recall for default:\", tp_default/predicted_default)\n",
    "            else:\n",
    "                print(\"Fail at recalling default\")\n",
    "            if predicted_ndefault != 0:\n",
    "                print(\"Recall for ndefault:\", tp_ndefault/predicted_ndefault)\n",
    "            else:\n",
    "                print(\"Fail at recalling ndefault\")\n",
    "            print(\"---------------------------------------------------------\")\n",
    "    \n",
    "        return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, voca_size, in_dim, hid_dim, word_embedding):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # Layers\n",
    "        #self.word2wemb = nn.Embedding(voca_size, rnn_in_dim).cuda()\n",
    "        #print(word_embedding.shape)\n",
    "        self.word2wemb = nn.Embedding.from_pretrained(torch.FloatTensor(word_embedding).to(device))\n",
    "        self.lstm = nn.LSTM(in_dim, hid_dim, bias = False, num_layers = 2, dropout = 0.9)\n",
    "        #self.rnn1 = nn.RNN(rnn_in_dim, rnn_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.rnn = nn.RNN(hid_dim, 15)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(15, 2)\n",
    "        #self.runlogit = nn.LogSoftmax(hid_dim, 2)\n",
    "\n",
    "    def init_rnn_hid(self):\n",
    "        \"\"\"Initial hidden state.\"\"\"\n",
    "        return torch.zeros(2, 1, self.hid_dim).cuda()\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"Feeds the words into the neural network and returns the value\n",
    "        of the output layer.\"\"\"\n",
    "        wembs = self.word2wemb(words) # (seq_len, rnn_in_dim)\n",
    "        lstm_outs, _ = self.lstm(wembs.unsqueeze(1))\n",
    "        #rnn_outs, _ = self.rnn1(wembs.unsqueeze(1), self.init_rnn_hid()) \n",
    "        #drop_outs = self.dropout(lstm_outs)\n",
    "        rnn_outs, _ = self.rnn(lstm_outs) \n",
    "                                      # (seq_len, 1, rnn_hid_dim)\n",
    "        #fc_outs = self.fc(drop_outs) # (1 x 3)\n",
    "        logit = self.fc(rnn_outs[-1])\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 67\n",
      "Number of testing examples: 17\n",
      "Building voca...\n",
      "n_voca: 11773\n",
      "Indexing words...\n",
      "Importing GloVe...\n",
      "Running classifier...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0354e4701368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running classifier...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#M = ClassifierRunner(data, len(word_idx), args.rnn_in_dim, args.rnn_hid_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifierRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# -epochs = 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-41ffb4a03046>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, voca_size, in_dim, hid_dim, word_embedding)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoca_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# criterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-00b2d4f6d3c2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, voca_size, in_dim, hid_dim, word_embedding)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#self.word2wemb = nn.Embedding(voca_size, rnn_in_dim).cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(word_embedding.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword2wemb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#self.rnn1 = nn.RNN(rnn_in_dim, rnn_hid_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "torch.cuda.device(0)\n",
    "#wnl = WordNetLemmatizer()\n",
    "start_time = time.time()\n",
    "print(\"Reading data...\")\n",
    "data_raw = defaultdict(list)\n",
    "voca_cnt = Counter()\n",
    "for text in train_df:\n",
    "    label = train_df[text]\n",
    "    words = word_tokenize(text.strip())\n",
    "    selected_words = []\n",
    "    for word in words:\n",
    "        if word in set(stopwords.words('english')):\n",
    "            continue\n",
    "        #word = wnl.lemmatize(word.lower())\n",
    "        selected_words.append(word)\n",
    "    #tagged = nltk.pos_tag(words)\n",
    "#     for (word, tag) in tagged:\n",
    "#         if tag in ['VBD', 'JJ']:\n",
    "#             target_words.append(word)\n",
    "#     if len(target_words) == 0:\n",
    "#         target_words.append(\"UNK\")\n",
    "#     data_raw[\"dev\"].append((words, label2idx[label.strip()]))\n",
    "#     voca_cnt.update(words)\n",
    "    # words -> target_words\n",
    "    data_raw[\"dev\"].append((selected_words, label2idx[label.strip()]))\n",
    "    voca_cnt.update(words)\n",
    "    \n",
    "for text in test_df:\n",
    "#     words = word_tokenize(text.strip())\n",
    "#     selected_words = []\n",
    "#     for word in words:\n",
    "#         if word in set(stopwords.words('english')):\n",
    "#             continue\n",
    "#         #word = wnl.lemmatize(word.lower())\n",
    "#         selected_words.append(word)\n",
    "#     words = word_tokenize(text.strip())\n",
    "    label = test_df[text]\n",
    "    data_raw[\"test\"].append((selected_words, label))\n",
    "    \n",
    "print(f'Number of training examples: {len(train_df)}')\n",
    "print(f'Number of testing examples: {len(test_df)}')\n",
    "\n",
    "print(\"Building voca...\")\n",
    "word_idx = {\"[UNK]\": 0}\n",
    "for word in voca_cnt.keys():\n",
    "    word_idx[word] = len(word_idx)\n",
    "print(\"n_voca:\", len(word_idx))\n",
    "\n",
    "print(\"Indexing words...\")\n",
    "data = defaultdict(list)\n",
    "for split in [\"dev\", \"test\"]:\n",
    "    for words, label in data_raw[split]:\n",
    "        data[split].append(([word_idx.get(w, 0) for w in words], label))\n",
    "\n",
    "print(\"Importing GloVe...\") \n",
    "\n",
    "with open('glove-2/glove.6B.300d.txt', encoding=\"utf-8\", mode=\"r\") as f:\n",
    "    word_embedding = np.random.normal(0, 1, (len(word_idx), 300))\n",
    "    \n",
    "    for line in f:\n",
    "        # Separate the values from the word\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "\n",
    "        # If word is in our vocab, then update the corresponding weights\n",
    "        idx = word_idx.get(word, None)\n",
    "        if word in word_idx:\n",
    "            word_embedding[idx] = np.array(line[1:], dtype=np.float32)\n",
    "\n",
    "print(\"Running classifier...\")\n",
    "#M = ClassifierRunner(data, len(word_idx), args.rnn_in_dim, args.rnn_hid_dim)\n",
    "M = ClassifierRunner(data, len(word_idx), 300, 80, word_embedding)\n",
    "\n",
    "# -epochs = 10\n",
    "for epoch in range(5):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "\n",
    "    # Train\n",
    "    M.run_epoch(\"dev\")\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        labels_pred = M.run_epoch(\"test\")\n",
    "        \n",
    "    with open(OUT_HELDOUT_PATH, \"w\") as f:\n",
    "        f.write(\"\\n\".join(labels_pred))\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed:\", round((end_time - start_time)/60, 2), 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change Nvidia Nsight\n",
    "torch.cuda.device(0)\n",
    "a = torch.cuda.FloatTensor(2).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
