{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchtext\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(BHF).txt',\n",
       " '(BTU).txt',\n",
       " '(CHC).txt',\n",
       " '(CLUB).txt',\n",
       " '(CVO).txt',\n",
       " '(CYH).txt',\n",
       " '(EVEP).txt',\n",
       " '(FE).txt',\n",
       " '(FELP).txt',\n",
       " '(FMSA).txt',\n",
       " '(FWM).txt',\n",
       " '(HERO).txt',\n",
       " '(I).txt',\n",
       " '(ICC).txt',\n",
       " '(ICON).txt',\n",
       " '(IHRT).txt',\n",
       " '(ISH).txt',\n",
       " '(NES).txt',\n",
       " '(PGN).txt',\n",
       " '(SFX).txt',\n",
       " '(SHLD).txt',\n",
       " '(WLB).txt',\n",
       " '.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " '._(BTU).txt',\n",
       " '._(CHC).txt',\n",
       " '._(HERO).txt',\n",
       " '._(PGN).txt',\n",
       " '._.DS_Store',\n",
       " 'preprocess']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "def load_data(data, label):\n",
    "    for file in os.listdir(label):\n",
    "        if file.endswith('.txt') and not file.startswith('.'):\n",
    "            file_path = label + '/' + file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data[text] = label\n",
    "df = {}\n",
    "load_data(df, 'default')\n",
    "load_data(df, 'ndefault')\n",
    "train_df = {}\n",
    "test_df = {}\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 11\n"
     ]
    }
   ],
   "source": [
    "import random, time\n",
    "random.seed(123)\n",
    "split = random.sample(list(df), k=int(len(df)*0.8))\n",
    "for s in split:\n",
    "    train_df[s] = df[s]\n",
    "for text in df:\n",
    "    if text not in train_df:\n",
    "        test_df[text] = df[text]\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_HELDOUT_PATH = \"pred_nn.txt\"\n",
    "\n",
    "idx2label = [\"default\", \"ndefault\"]\n",
    "label2idx = {label: idx for idx, label in enumerate(idx2label)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierRunner(object):\n",
    "    def __init__(self, data, voca_size, in_dim, hid_dim, word_embedding):\n",
    "        self.data = data\n",
    "        self.clf = Classifier(voca_size, in_dim, hid_dim, word_embedding)\n",
    "        self.optimizer = optim.Adam(self.clf.parameters())\n",
    "        # criterion\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        #self.ce_loss = nn.PoissonNLLLoss()\n",
    "\n",
    "    def run_epoch(self, split):\n",
    "        \"\"\"Runs an epoch, during which the classifier is trained or applied\n",
    "        on the data. Returns the predicted labels of the instances.\"\"\"\n",
    "\n",
    "        if split == \"dev\": self.clf.train()\n",
    "        else: self.clf.eval()\n",
    "        epoch_loss = 0\n",
    "        labels_pred = []\n",
    "        for i, (words, label) in enumerate(self.data[split]):\n",
    "            #m = nn.Dropout(p=0.9, inplace = True)\n",
    "            logit = self.clf(torch.LongTensor(words))\n",
    "            #logit = m(logit)\n",
    "            # Optimize\n",
    "            if split == \"dev\":\n",
    "                loss = self.ce_loss(logit, torch.LongTensor([label]))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                #print(loss.item())\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            idx = torch.argmax(logit)\n",
    "            #print(idx)\n",
    "            #print(\"------------\")\n",
    "            \n",
    "            labels_pred.append(idx2label[idx])\n",
    "        if split == \"test\":\n",
    "            tp_default = 0\n",
    "            tp_ndefault = 0\n",
    "            total_default = 0\n",
    "            total_ndefault = 0\n",
    "            predicted_default = 0\n",
    "            predicted_ndefault = 0\n",
    "            total = len(self.data[split])\n",
    "\n",
    "            for i in range(len(self.data[split])):\n",
    "                _, test_label = self.data[split][i]\n",
    "                \n",
    "                print(\"Lable:\", test_label)\n",
    "                print(\"Pridicted:\", labels_pred[i])\n",
    "                \n",
    "                if test_label == \"default\":\n",
    "                    total_default += 1\n",
    "                    if labels_pred[i] == \"default\":\n",
    "                        tp_default += 1\n",
    "                else:\n",
    "                    total_ndefault += 1\n",
    "                    if labels_pred[i] == \"ndefault\":\n",
    "                        tp_ndefault += 1\n",
    "                if labels_pred[i] == \"default\":\n",
    "                    predicted_default += 1\n",
    "                else:\n",
    "                    predicted_ndefault += 1\n",
    "            print(\"Overall accuracy:\", (tp_default+tp_ndefault)/total)\n",
    "            print(\"Precision for default:\", tp_default/total_default)\n",
    "            print(\"Precision for ndefault:\", tp_ndefault/total_ndefault)\n",
    "            if predicted_default != 0:\n",
    "                print(\"Recall for default:\", tp_default/predicted_default)\n",
    "            else:\n",
    "                print(\"Fail at recalling default\")\n",
    "            if predicted_ndefault != 0:\n",
    "                print(\"Recall for ndefault:\", tp_ndefault/predicted_ndefault)\n",
    "            else:\n",
    "                print(\"Fail at recalling ndefault\")\n",
    "            print(\"---------------------------------------------------------\")\n",
    "    \n",
    "        return labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, voca_size, in_dim, hid_dim, word_embedding):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # Layers\n",
    "        #self.word2wemb = nn.Embedding(voca_size, rnn_in_dim).cuda()\n",
    "        #print(word_embedding.shape)\n",
    "        self.word2wemb = nn.Embedding.from_pretrained(torch.FloatTensor(word_embedding))\n",
    "        self.lstm = nn.LSTM(in_dim, hid_dim, bias = False, num_layers = 2, dropout = 0.9)\n",
    "        #self.rnn1 = nn.RNN(rnn_in_dim, rnn_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.rnn = nn.RNN(hid_dim, 15)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(15, 2)\n",
    "        #self.runlogit = nn.LogSoftmax(hid_dim, 2)\n",
    "\n",
    "    def init_rnn_hid(self):\n",
    "        \"\"\"Initial hidden state.\"\"\"\n",
    "        return torch.zeros(2, 1, self.hid_dim)\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"Feeds the words into the neural network and returns the value\n",
    "        of the output layer.\"\"\"\n",
    "        wembs = self.word2wemb(words) # (seq_len, rnn_in_dim)\n",
    "        lstm_outs, _ = self.lstm(wembs.unsqueeze(1))\n",
    "        #rnn_outs, _ = self.rnn1(wembs.unsqueeze(1), self.init_rnn_hid()) \n",
    "        #drop_outs = self.dropout(lstm_outs)\n",
    "        rnn_outs, _ = self.rnn(lstm_outs) \n",
    "                                      # (seq_len, 1, rnn_hid_dim)\n",
    "        #fc_outs = self.fc(drop_outs) # (1 x 3)\n",
    "        logit = self.fc(lstm_outs[-1])\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of training examples: 43\n",
      "Number of testing examples: 11\n",
      "Building voca...\n",
      "n_voca: 15732\n",
      "Indexing words...\n",
      "Importing GloVe...\n",
      "Running classifier...\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 80], m2: [15 x 2] at C:\\w\\1\\s\\tmp_conda_3.7_104508\\conda\\conda-bld\\pytorch_1572950778684\\work\\aten\\src\\TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-90beda5f9c5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dev\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-c3edd537a5ee>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(self, split)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#m = nn.Dropout(p=0.9, inplace = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;31m#logit = m(logit)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# Optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-49de57dc7ab0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     35\u001b[0m                                       \u001b[1;31m# (seq_len, 1, rnn_hid_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#fc_outs = self.fc(drop_outs) # (1 x 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_outs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 80], m2: [15 x 2] at C:\\w\\1\\s\\tmp_conda_3.7_104508\\conda\\conda-bld\\pytorch_1572950778684\\work\\aten\\src\\TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "start_time = time.time()\n",
    "print(\"Reading data...\")\n",
    "data_raw = defaultdict(list)\n",
    "voca_cnt = Counter()\n",
    "for text in train_df:\n",
    "    label = train_df[text]\n",
    "    words = word_tokenize(text.strip())\n",
    "    selected_words = []\n",
    "    for word in words:\n",
    "        if word in set(stopwords.words('english')):\n",
    "            continue\n",
    "        #word = wnl.lemmatize(word.lower())\n",
    "        selected_words.append(word)\n",
    "    #tagged = nltk.pos_tag(words)\n",
    "#     for (word, tag) in tagged:\n",
    "#         if tag in ['VBD', 'JJ']:\n",
    "#             target_words.append(word)\n",
    "#     if len(target_words) == 0:\n",
    "#         target_words.append(\"UNK\")\n",
    "#     data_raw[\"dev\"].append((words, label2idx[label.strip()]))\n",
    "#     voca_cnt.update(words)\n",
    "    # words -> target_words\n",
    "    data_raw[\"dev\"].append((selected_words, label2idx[label.strip()]))\n",
    "    voca_cnt.update(words)\n",
    "    \n",
    "for text in test_df:\n",
    "    words = word_tokenize(text.strip())\n",
    "    for word in words:\n",
    "        if word in set(stopwords.words('english')):\n",
    "            continue\n",
    "        word = wnl.lemmatize(word.lower())\n",
    "        selected_words.append(word)\n",
    "    words = word_tokenize(text.strip())\n",
    "    label = test_df[text]\n",
    "    data_raw[\"test\"].append((selected_words, label))\n",
    "    \n",
    "print(f'Number of training examples: {len(train_df)}')\n",
    "print(f'Number of testing examples: {len(test_df)}')\n",
    "\n",
    "print(\"Building voca...\")\n",
    "word_idx = {\"[UNK]\": 0}\n",
    "for word in voca_cnt.keys():\n",
    "    word_idx[word] = len(word_idx)\n",
    "print(\"n_voca:\", len(word_idx))\n",
    "\n",
    "print(\"Indexing words...\")\n",
    "data = defaultdict(list)\n",
    "for split in [\"dev\", \"test\"]:\n",
    "    for words, label in data_raw[split]:\n",
    "        data[split].append(([word_idx.get(w, 0) for w in words], label))\n",
    "\n",
    "print(\"Importing GloVe...\") \n",
    "\n",
    "with open('glove-2/glove.6B.300d.txt', encoding=\"utf-8\", mode=\"r\") as f:\n",
    "    word_embedding = np.random.normal(0, 1, (len(word_idx), 300))\n",
    "    \n",
    "    for line in f:\n",
    "        # Separate the values from the word\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "\n",
    "        # If word is in our vocab, then update the corresponding weights\n",
    "        idx = word_idx.get(word, None)\n",
    "        if word in word_idx:\n",
    "            word_embedding[idx] = np.array(line[1:], dtype=np.float32)\n",
    "\n",
    "print(\"Running classifier...\")\n",
    "#M = ClassifierRunner(data, len(word_idx), args.rnn_in_dim, args.rnn_hid_dim)\n",
    "M = ClassifierRunner(data, len(word_idx), 300, 80, word_embedding)\n",
    "\n",
    "# -epochs = 10\n",
    "for epoch in range(5):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "\n",
    "    # Train\n",
    "    M.run_epoch(\"dev\")\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        labels_pred = M.run_epoch(\"test\")\n",
    "        \n",
    "    with open(OUT_HELDOUT_PATH, \"w\") as f:\n",
    "        f.write(\"\\n\".join(labels_pred))\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed:\", round((end_time - start_time)/60, 2), 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
